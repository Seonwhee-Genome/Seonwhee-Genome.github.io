---
title: "RoBERTa"
tags: [NLP]
categories:
  - Blogs_Korean_NLP
date: 2020-12-02
comments: true
use_math: true
---

> Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.

BERT와 같은 모델을 학습시키는 것은 컴퓨팅이 많이 필요해서 가능한 튜닝 가짓수를 제한하고, 각기 다른 사이즈의 private data를 학습하는 경우가 많아서 모델의 발전에 의한 객관적인 효과 측정을 제한한다.<br><br>
> We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. <span style="color:orange">We find that BERT was significantly undertrained</span> and propose <span style="color:blue">an improved recipe for training BERT models, which we call RoBERTa</span>, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: <span style="color:blue">(1) training the model longer, with bigger batches, over more data</span>; <span style="color:blue">(2) removing the next sentence prediction objective</span>; <span style="color:blue">(3) training on longer sequences</span>; and <span style="color:blue">(4) dynamically changing the masking pattern applied to the training data.</span> We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects.
<br>

하이퍼파라미터 튜닝과 학습데이터 사이즈에 따른 효과를 평가하는 BERT pretraining 연구를 했고, BERT가 심각하게 덜 학습된 것을 발견했다.<br>

그래서 RoBERTa라고 하는 개선된 BERT 학습 방법을 제시하였다. 4가지를 기존 BERT에서 바꿨는데, (1) 더 많은 데이터를, 더 큰 배치로 더 오래 학습시키고, (2) NSP objective를 제거했고, (3) 길이가 더 긴 문장 토큰들을 학습시켰고, (4) 동적인 Masking pattern을 학습데이터에 적용하였다.<br><br>

> Specifically, RoBERTa is trained with <span style="color:orange">dynamic masking</span> (Section 4.1), <span style="color:orange">FULL-SENTENCES without NSP loss</span> (Section 4.2), <span style="color:orange">large mini-batches</span> (Section 4.3) and <span style="color:orange">a larger byte-level BPE</span> (Section 4.4).
Additionally, we investigate two other important factors that have been under-emphasized in previous work: <span style="color:green">(1) the data used for pretraining</span>, and <span style="color:green">(2) the number of training passes through the data.</span> For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT.

구체적으로 RoBERTa는 BERT에서 1) dynamic masking으로 masking 했고, 2) NSP loss 없이 FULL-SENTENCES 형태의 input을 사용했고, 3) 미니배치 사이즈를 크게 했다. 4) 더 큰 vocab size를 기준으로 BPE을 했다.<br>
덤으로 Pretraining할 때 사용하는 data의 종류와 training epoch 수를 달리해서 테스트를 해봤다.<br><br>

BERT 논문(Devlin et al., 2019) Ablation Study 결과에서<br>
<img src="/assets/img_nlp/BERT_table5.png" alt="bert_table" width="450" /><br>
NSP를 제거한 경우에 QNLI, MNLI같은 NLI와 SQuAD 1.1같은 Q&A Task에서 유의미한 성능저하가 일어났다고 서술하고 있다.<br>

> In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1.

BERT base와 No NSP를 보면 QNLI 같은 경우는 성능 저하가 꽤 큰 폭으로 일어 났음을 알 수 있지만, 나머지 Task는 그닥 두드러지지 않는데, 실제로 이후 연구자들이 NSP를 꼭 Pre-training에 포함시켜야 하느냐 하는지에 관하여 회의적인 시각을 갖게 된다. <br><br>

다시 RoBERTa 논문으로 와서 Model Input 형식과 NSP 학습 여부를 달리해서 비교 실험한 결과를 보면<br>
<img src="/assets/img_nlp/RoBERTa_result.png" alt="bert_table" width="450" /><br>
우선 RoBERTa 연구자들이 테스트한 Input format은 다음과 같다.<br>
<img src="/assets/img_nlp/RoBERTa_input.png" alt="bert_table" width="650" /><br><br>

>We find that <span style="color:orange">using individual sentences hurts performance on downstream tasks</span>, which we hypothesize is because the model is not able to learn long-range dependencies.
We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published $BERT_{BASE}$ results and that <span style="color:orange">removing the NSP loss matches or slightly improves downstream task performance</span>, in contrast to Devlin et al. (2019). It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format.<br>
Finally we find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULL-SENTENCES in the remainder of our experiments for easier comparison with related work.

첫번째는 문장 단위로 Pair를 구성해서 BERT input으로 했을 때 길이가 너무 짧기 때문에 토큰들의 넓은 범위의 의존성을 학습하지 못해서 Downstream task 성능이 저하된다고 하며,
가장 중요한 결론은 <span style="color:orange">NSP loss를 쓰지 않아도, 성능은 충분히 나온다</span>는 것이다. Input format은 동일한 주제를 가진 Document 범위 안에서 512 토큰을 맞추는(DOC-SENTENCES)가 좋지만, 배치사이즈가 들쭉 날쭉해지므로 편의상 RoBERTa를 비롯해서 다른 작업들은 Document 구분 없이 512 토큰을 맞추는 FULL-SENTENCES 방식으로 하였다.<br><br><br>

<script src="https://utteranc.es/client.js"
        repo="Seonwhee-Genome/Seonwhee-Genome.github.io"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>