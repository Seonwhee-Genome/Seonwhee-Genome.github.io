---
title: "GPT"
tags: [NLP]
categories:
  - Blogs_Korean_NLP
date: 2021-07-11
comments: true
use_math: true
---

BERT 논문의 상당부분이 GPT 및 ELMo와 비교하는데 할애된 것은 이미 잘 알고 있을 것이다. 그리고 GPT가 Autoregressive Language Model인 것도 이미 알고 있으며, 특히 XLNet이 Permutation Autoregressive Language Model로서 GPT 계열의 모델에 영감을 받은 점도 알 것이다.  
GPT는 Transformer와 BERT 사이에 나온 모델이지만, 단순히 Decoder를 사용한 언어모델, 그 이상의 의미가 있다.  
Abstract는  
> Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, <span style="color:orange">labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately.</span> 

로 시작하는데, 해석해보면  
"자연어 이해라고 하는 것은 TE, QA, 의미론적 유사성 평가, 문서분류 같은 매우 다양한 태스크들을 포괄하고 있다. 라벨링 되지 않은 말뭉치 자체는 엄청 풍부하더라도, <u>특정 태스크를 위한 라벨링 데이터가 적다는 점은 태스크별로 구분지어(특화시켜) 학습된 모델이 제대로 작동하는데 걸림돌이 된다.</u>"  
한마디로 자연어 분야에는 수많은 태스크들이 있고, 그러다보니 <span style="color:orange">태스크별로 라벨링된 데이터는 극히 일부에 불과하다</span>는 점이 문제임을 알 수 있다.  
그리고 이런 문제에도 불구하고 <span style="color:orange">어떻게든 태스크 특화 모델이 잘 작동하게 만드는 것</span>이 이 논문의 목표라는 것도 어렵지 않게 추측할 수 있다. 
그래서  
> We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.   

"이런 NLU task들에서의 성취는 라벨링 되지 않은 다양한 말뭉치로 언어모델을 generative pre-training을 시킨 다음, 각 특정 task에 대한 discriminative fine-tuning을 함으로써 이뤄낼 수 있다"  
    
거인의 어깨에 올라타는 <span style="color:orange">Pre-train & Fine-tuning </span>방법론, BERT에서 봤던 내용이다. BERT보다 이 GPT 논문이 먼저 나왔고, BERT 논문 Introduction 바로 첫문장에서 이 논문을 인용하고 있다.  
> Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; <span style="color:green">Radford et al., 2018</span>; Howard and Ruder, 2018).     

인용 리스트를 잘 보면 Dai and Le, 2015가 있는 것으로 봐서 이미 2015년에 NLP에서 Pre-training & Fine-tuning의 아이디어가 나왔음을 알 수 있다.  
다시 GPT 논문의 Abstract로 돌아와서  
> In contrast to previous approaches, we make <span style="color:orange">use of task-aware input transformations during fine-tuning</span> to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding.   

뒤의 문장은 바로 자신들이 하기로 한 Generative pre-training + Discriminative fine-tuning의 효율성을 설명하고 있으며, 이런 접근 방식이 다양한 NLU 벤치마크에서 효과를 보였다고 한다. task-aware input transformation이라고 하는 말은 파인튜닝시 다운스트림 태스크에 적합하게 입력 형태를 변형시켜주는 것을 의미한다. 이것에 대해서는 논문 3.3에 설명이 되어 있다.  

> Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).  
  
벤치마크에서의 효과가 뭐였는지 설명하는 문장이다. general task-agnostic model은 pre-trained + fine-tuning model로 생각하면 되고, 바로 GPT model이다. 이 모델이 task별로 따로 아키텍쳐를 설계하여 학습시킨 모형들보다 더 성능이 좋았는데, 12개 과제 중에 9개에서 SOTA를 기록할 정도의 엄청난 향상이 있었다고 한다. 예를 들어서 상식 문제인 Stories Cloze Test에서는 8.9%, QA인 RACE에서는 5.7%, TE인 MultiNLI에서는 1.5%의 향상을 보였다고 한다.  

초록을 읽어본 결과, 여기까지 읽었을 때 우리가 봐왔던 여느 자연어 Pre-training 모델들과 같이 <span style="color:orange">Generative pre-training(GPT) + Discriminative fine-tuning가 효과적인 방법론이다</span>라는 것이 이 논문의 주제 같아 보이는데 과연 그럴까? 다시 말하지만 이 논문은 BERT 보다 먼저 나왔다.

