---
title: "GPT"
tags: [NLP]
categories:
  - Blogs_Korean_NLP
date: 2021-07-11
comments: true
use_math: true
---

BERT 논문의 상당부분이 GPT 및 ELMo와 비교하는데 할애된 것은 이미 잘 알고 있을 것이다. 그리고 GPT가 Autoregressive Language Model인 것도 이미 알고 있으며, 특히 XLNet이 Permutation Autoregressive Language Model로서 GPT 계열의 모델에 영감을 받은 점도 알 것이다.  
GPT는 Transformer와 BERT 사이에 나온 모델이지만, 단순히 Decoder를 사용한 언어모델, 그 이상의 의미가 있다.  
Abstract는  
> Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, <span style="color:orange">labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately.</span> 

로 시작하는데, 해석해보면  
"자연어 이해라고 하는 것은 TE, QA, 의미론적 유사성 평가, 문서분류 같은 매우 다양한 태스크들을 포괄하고 있다. 라벨링 되지 않은 말뭉치 자체는 엄청 풍부하더라도, <u>특정 태스크를 위한 라벨링 데이터가 적다는 점은 태스크별로 구분지어(특화시켜) 학습된 모델이 제대로 작동하는데 걸림돌이 된다.</u>"  
한마디로 자연어 분야에는 수많은 태스크들이 있고, 그러다보니 <span style="color:orange">태스크별로 라벨링된 데이터는 극히 일부에 불과하다</span>는 점이 문제임을 알 수 있다.  
그리고 이런 문제에도 불구하고 <span style="color:orange">어떻게든 태스크 특화 모델이 잘 작동하게 만드는 것</span>이 이 논문의 목표라는 것도 어렵지 않게 추측할 수 있다.  