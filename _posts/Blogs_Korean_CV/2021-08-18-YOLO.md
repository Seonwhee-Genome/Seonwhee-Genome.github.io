---
title: "YOLO"
tags: [Vision]
categories:
  - Blogs_Korean_CV
date: 2021-08-18
comments: true
use_math: true
---


<img src="/assets/img_cv/YOLO_output.png" alt="YOLO output" width="50" /><br>
YOLO 네트워크(darknet)의 Output은 상당히 독창적이다. 아래와 같은 Input 이미지를 사전에 지정한 $S \times S$격자(Grid cell)로 추상화해서 각 Grid cell이 갖고 있는 정보들(GT 좌표, 클래스)을 $B \times 5 + C$개의 숫자로 예측하도록 Output representation을 디자인 하였다.<br><br>
<img src="/assets/img_cv/YOLO_input.png" alt="YOLO input" width="250" /><br>
이렇게 Output representation을 task에 맞게 설계하는 아이디어는 Pre-trained CNN을 활용해서 무언가를 할 때 영감을 준다.<br><br>
YOLO는 하나의 네트워크 아키텍쳐로 이미지를 그리드로 나눠서 단일한 손실함수를 통해 각 그리드 마다 Classification와 localization을 병렬적으로 동시에 계산함으로써 빠르게 학습이 가능하다. <br><br>
<img src="/assets/img_cv/YOLO_figure3.png" alt="YOLO figure3" width="550" /><br>

YOLO의 Detection을 위한 아키텍쳐이고, GoogleNet에서 영감을 받았지만, 정작 Inception module이 들어가지는 않았고, 1x1 reduction layer들을 사용했다. 이 아키텍쳐를 바로 Detection task로 학습 시킨 것이 아니라 ImageNet의 1000개 클래스 분류문제를 푸는 Pre-train을 시켜 놓고, 그 다음에 Detection Training을 하였다.<br><br>

Pre-training는 위 아키텍쳐의 처음 20개 conv layer에다가 average pooling과 fully connected layer 하나씩 붙여서(아래 그림) 일주일간 진행하였다.<br>
<img src="/assets/img_cv/YOLO_figure3a.png" alt="YOLO figure3a" width="550" /><br>
<br>
20개의 Pretrained conv layer들을 가지고 Detection 전이학습을 하기 위해서 아래의 layer들이 추가되어 학습되는데<br>
<img src="/assets/img_cv/YOLO_figure3b.png" alt="YOLO figure3" width="250" /><br>

최종적인 Output 형태가 7x7x30의 텐서임을 알 수 있다. 이미 우리가 잘 알고 있듯이 7x7은 Grid cell이라는 이미지 상의 격자공간이고 30개의 feature들이 Bounding box 예측좌표, class 예측정보, confidence score를 의미한다. Grid cell을 기준으로 예측값이 나오기 때문에 Ground truth 역시 Grid cell 기준으로 값을 바꿔줘야 함을 알 수 있다.<br><br>

참고로 그리드별 바운딩 박스 정보는 객체의 중심은 각 그리드에 대한 상대적인 위치, 가로 세로는 전체 이미지에 대한 상대적인 길이로 representation된다.<br>

결과적으로 7X7 그리드에 바운딩 박스 정보, 클래스 확률 정보가 longtitudinal 하게 저장된 형태의 텐서를 가지게 되고<br>

이 텐서 값들이 손실함수에서 손실 값을 구하는 데 사용된다.<br><br>
<img src="/assets/img_cv/YOLO_figure2.png" alt="YOLO figure2" width="550" /><br>

YOLO같은 경우는 Grid Cell 하나마다 두개의 바운딩박스를 허용하여 객체를 잡도록 되어 있고, <span style="color:orange">바운딩박스에 대한 Confidence score ( $Pr(Object)\cdot IOU^{truth}_{pred}$ ) </span>가 Threshold를 넘었는지 확인해서 Grid Cell에 중심점을 둔 바운딩 박스 안에 객체가 있는지 판단한다.  위의 그림에서 알 수 있듯이 Bounding box localization이 어떻게 되는지와 무관하게 모든 Grid cell에 대해서 <span style="color:orange">Grid cell 기준으로 Class의 조건부확률 $Pr(Class_i|Object)$ </span>를 구한다.  
  
YOLO의 손실함수는 위치, 신뢰도, 클래스 세가지 항목의 Squared error를 Weighted Sum한 SSE(Sum Squared Error)를 사용한다.  

$\lambda_{coord} = 5, \ \lambda_{noobj}=.5$  <br>
$i$ 는 Grid Cell의 인덱스, $j$ 는 Grid $i$ 에 중심점이 있는 Bounding Box의 인덱스이다.<br><br>

>We optimize for <span style="color:orange">sum-squared error</span> in the output of our model. We use sum-squared error because it is easy to optimize, however it <span style="color:orange">does not perfectly align with our goal of maximizing average precision.</span> It <span style="color:green">weights localization error equally with classification error</span> which may not be ideal. Also, in every image <span style="color:green">many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability</span>, causing training to diverge early on.
To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\lambda_{coord}$ and $\lambda_{noobj}$ to accomplish this. We set $\lambda_{coord} = 5$ and $\lambda_{noobj}=.5$ .
Sum-squared error also equally weights errors in large boxes and small boxes. <span style="color:orange">Our error metric should reflect that small deviations in large boxes matter less than in small boxes.</span> To partially address this we predict <span style="color:orange">the square root of the bounding box width and height</span> instead of the width and height directly.

위의 손실함수에서 $\lambda_{coord}$ 을 둔 이유는 Bounding Box Localization과 Classification의 가중치를 다르게 하기 위해서이고, $\lambda_{noobj}$ 를 둔 이유는 객체가 없는 Grid cell이 많을 것이고 이 경우 손실함수는 $\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}^{noobj}_{i\ j}(C_i-\hat{C}_i)^2$ 만 남게 되는데 객체가 없더라도 Grid cell에는 바운딩 박스 2개 씩은 예측되므로, 이 경우 Confidence score인 $\hat{C}_i$ 가 0에 가까운 값이 되며, 객체가 없는 Grid cell의 영향력이 객체가 있는 Grid cell의 영향력보다 압도적으로 커지는 경우도 종종 생겨서 이를 방지하기 위함이다.<br>

또한 큰 사이즈의 바운딩 박스($\hat{w}_i$ 와 $\hat{h}_i$ 의 값이 큰 경우)는 작은 사이즈의 바운딩 박스보다는 오차의 크기에 대해 조금 덜 민감해도 괜찮기 때문에 제곱근을 취해준다.<br><br>

>YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. <span style="color:orange">We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth.</span> This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.

Grid cell마다 두 개의 바운딩박스가 예측되는데( $B=2$ ). 객체 하나에 바운딩 박스는 하나만 있으면 되므로, 둘 중에 IoU가 높은 것을 선택해서 $\mathbb{1}^{obj}_{i\ j}$ 값을 1로 주고 loss를 계산하게 된다.<br><br><br>

Inference 단계에서는 GT가 없기 때문에 $IOU_{pred}^{truth}$ 를 통해서 중복되는 Bounding box를 제거할 수 없다.<br>

따라서 Inference 시에는 중복되는 바운딩 박스를 제거하기 위해서 먼저 위에서 언급한 Class의 조건부확률과 Confidence score를 곱해서 Grid cell에서 특정 class에 대한 바운딩 박스의 confidence score(Class-specific confidence score)를 구한다.  

$Pr(Class_i \| Object) \cdot Pr(Object) \cdot IOU^{truth}_{pred}$    

$ = Pr(Class_i) \cdot IOU_{pred}^{truth} $

수식으로 따지면 위와 같지만, 말했듯 Inference에선 $IOU_{pred}^{truth}$는 알 수 없기 때문에 Non Maximum Suppression(NMS)으로 중복되는 바운딩박스를 제거하며, NMS에서의 IoU는 GT BB가 아닌, Predicted Bounding Box들을 가지고 구하는 점을 기억하자.<br><br><br>
<img src="/assets/img_cv/YOLO_table1.png" alt="YOLO table1" width="550" /><br>
YOLO의 강점은 Inference 속도에 있다. YOLO가 Faster R-CNN보다 mAP마저 우월했다면 굳이 Less Than Real-Time을 따로 분류하지는 않았을 것이다.<br><br> 

저자는 Table 1에서 의도적으로 속도를 부각시킨다. 실제로 Faster R-CNN도 Frame per Second가 7, 18이면 상당히 느린 축에 속하는데 Fast R-CNN은 0.5 FPS로 Pascal VOC 2007 이미지 하나 처리하는데 2초가 걸리는 심각한 문제가 있음을 알 수 있고, 덕분에 저자가 YOLO의 경쟁력을 강조할 논리적 근거가 생겼다.<br><br>
<img src="/assets/img_cv/YOLO_figure4.png" alt="YOLO figure4" width="550" /><br>
위와 같이 Error의 Type을 분석하는 것은 알고리즘의 장단점을 파악하고 비교하는데 도움이 된다.<br>

YOLO의 경우 Localization(분류는 정확히 했는데, IoU가 0.5도 안되서 바운딩박스 예측에 실패)와 Other(분류를 잘못함) 에러가 높았고<br>

Fast R-CNN은 Background(객체가 없는데 바운딩박스를 예측)에러가 많았다는 특징이 있다.<br>

Table 2의 실험은 Fast R-CNN이 Background Error만 개선하면 더 좋은 모델이 될 수 있고, YOLO를 통해서 개선할 수 있지 않을까 하는 가설을 검증한 것이다. 다시 말하면 YOLO가 Background error가 적다는 장점이자 특징을 간접적으로 검증하기 위한 목적이 있다.<br>
<img src="/assets/img_cv/YOLO_table2.png" alt="YOLO table2" width="550" /><br>
Fast R-CNN이 예측한 Bounding box가 YOLO의 예측한 것과 비슷하고 겹치면 그 바운딩박스를 채택하게 하는 방식으로 앙상블을 하는 것이고, 마찬가지 방식으로 여러 버전의 Fast R-CNN을 앙상블을 했는데, YOLO와의 앙상블의 mAP 상승효과가 가장 컸다.<br><br> 

이 실험은 앞서 언급했듯  YOLO의 에러 양상을 검증하기 위한 목적이 강하고, 이 앙상블을 쓰기 위한 목적이라고 하기는 어렵다. 앙상블의 방식이 각각의 모델들을 독립적으로 적용해서 결과물을 가지고 결합하는 것이기 때문에 속도가 너무 느린 Fast R-CNN을 쓸 수 없기 때문이다.<br><br><br>

<script src="https://utteranc.es/client.js"
        repo="Seonwhee-Genome/Seonwhee-Genome.github.io"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>